{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NFR2 import NFR\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yooo\n",
      "**********************************\n",
      "[                identity                                      hash  target_x  \\\n",
      "0  DatabaseFR\\hamza1.png  cc01ee63c672464aa33e6452d82ae10443b43c02        96   \n",
      "\n",
      "   target_y  target_w  target_h  source_x  source_y  source_w  source_h  \\\n",
      "0       259       572       572       106       253       246       246   \n",
      "\n",
      "   threshold  distance  \n",
      "0        0.4  0.339913  ]\n",
      "Employee name: hamza1 \n",
      "coordinates: 106,253,246,246,352,499\n",
      "distance = 0.33991251920698085\n"
     ]
    }
   ],
   "source": [
    "image_path = \"D://Retinafacetest/h.jpg\"\n",
    "name, xmin, ymin, w, h, xmax, ymax,dis = NFR.FindFaceFromImage(image_path,showlog=True)\n",
    "\n",
    "print(f\"Employee name: {name} \\ncoordinates: {xmin},{ymin},{w},{h},{xmax},{ymax}\\ndistance = {dis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **new input**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - input : image with bounding boxes array \n",
    "# - output: new croped images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "def crop_images(image_path, bounding_boxes):\n",
    "    cropped_images = []\n",
    "    # to clear all of the folder past contents (we dont need the previous croped images right?)\n",
    "    files = glob.glob(os.path.join(\"Croppedimages\", '*'))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "    with Image.open(image_path) as img:\n",
    "        for box in bounding_boxes:\n",
    "            x, y, w, h = box\n",
    "            crop_box = (x, y, x + w, y + h)\n",
    "            cropped_img = img.crop(crop_box)\n",
    "            cropped_images.append((cropped_img,(x,y,w,h)))\n",
    "            # cropped_img.save(os.path.join(\"Croppedimages/\", f\"cropped_image_{i}.jpg\"))\n",
    "            filename = f\"cropped_image.jpg\"\n",
    "            new_image_path = os.path.join(\"Croppedimages/\", filename)\n",
    "            increment = 1\n",
    "            \n",
    "            while os.path.exists(new_image_path):\n",
    "                new_image_path = os.path.join(\"Croppedimages/\", f\"cropped_image_{increment}.jpg\")\n",
    "                increment += 1\n",
    "                \n",
    "            # Save the cropped image\n",
    "            cropped_img.save(new_image_path)\n",
    "    print(cropped_images)\n",
    "    return cropped_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *a test code for the crop*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<PIL.Image.Image image mode=RGB size=212x38 at 0x2016A577200>, (62, 237, 212, 38)), (<PIL.Image.Image image mode=RGB size=150x150 at 0x2016A5770E0>, (300, 300, 150, 150))]\n",
      "<PIL.Image.Image image mode=RGB size=150x150 at 0x2016A5770E0>\n"
     ]
    }
   ],
   "source": [
    "bounding_boxes = [(62,237,212,38), (300, 300, 150, 150)]\n",
    "cropped_imgs = crop_images('D://Retinafacetest/three.jpg',bounding_boxes)\n",
    "print(cropped_imgs[1][0])\n",
    "\n",
    "new_images=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **New function call**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_images.append(f\"cropped_image2.jpg\")\n",
    "# print(new_images)\n",
    "# new_images2=[(\"cropped_image0.jpg\",(62,237,212,38)),(\"cropped_image1.jpg\",(300, 300, 150, 150)),(\"cropped_image2.jpg\",(4,237,212,38))]\n",
    "# def FaceRec(showlog):\n",
    "#     findings= list([],)\n",
    "#     for i in new_images2:\n",
    "#         print(i)\n",
    "#         print(\"\\n----------------------------------------------\\n\")\n",
    "#         print(f\"Image {i} : \\n\")\n",
    "#         newfindings=NFR.FindFaceFromImage(i,showlog=True)  \n",
    "#         print(f\"neew finding:\\n{newfindings}\\n\")\n",
    "#         findings.append(newfindings)\n",
    "#         print(\"\\n----------------------------------------------\\n\")\n",
    "#     return findings\n",
    "\n",
    "\n",
    "# with out pp\n",
    "def FaceRec(new_images2, showlog):\n",
    "    updated_images = []\n",
    "    \n",
    "    for i, (image_path, coords) in enumerate(new_images2):\n",
    "        print(image_path)\n",
    "        print(\"\\n----------------------------------------------\\n\")\n",
    "        print(f\"Image {image_path} : \\n\")\n",
    "        \n",
    "        # Assume NFR.FindFaceFromImage takes an image path and a showlog parameter\n",
    "        newfindings = NFR.FindFaceFromImage(image_path, showlog=showlog)\n",
    "        \n",
    "        print(f\"new finding:\\n{newfindings}\\n\")\n",
    "        updated_images.append((image_path, coords, newfindings))\n",
    "        \n",
    "        print(\"\\n----------------------------------------------\\n\")\n",
    "    \n",
    "    return updated_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D://Retinafacetest/three1.jpg\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "Image D://Retinafacetest/three1.jpg : \n",
      "\n",
      "24-07-03 18:30:07 - Found 7 newly added image(s), 7 removed image(s), 0 replaced image(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding representations:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding representations: 100%|██████████| 7/7 [00:14<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-07-03 18:30:21 - There are now 8 representations in Database.pkl\n",
      "24-07-03 18:30:21 - Searching D://Retinafacetest/three1.jpg in 8 length datastore\n",
      "24-07-03 18:30:23 - find function duration 15.633016347885132 seconds\n",
      "new finding:\n",
      "['Noore', 28, 87, 162, 162, 190, 249, 0.2882234397537323]\n",
      "\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "[('D://Retinafacetest/three1.jpg', (4, 237, 212, 38), ['Noore', 28, 87, 162, 162, 190, 249, 0.2882234397537323])]\n"
     ]
    }
   ],
   "source": [
    "new_images2 = [\n",
    "    # (\"cropped_image0.jpg\", (62, 237, 212, 38)),\n",
    "    # (\"cropped_image1.jpg\", (300, 300, 150, 150)),\n",
    "    # (\"cropped_image2.jpg\", (4, 237, 212, 38)),\n",
    "    # (\"D://Retinafacetest/ms.jpg\", (4, 237, 212, 38)),\n",
    "    # (\"D://Retinafacetest/dar.jpg\", (4, 237, 212, 38)),\n",
    "    (\"D://Retinafacetest/three1.jpg\", (4, 237, 212, 38)),\n",
    "    # (\"D://Retinafacetest/h.jpg\", (4, 237, 212, 38)),\n",
    "    \n",
    "    \n",
    "    \n",
    "]\n",
    "showlog = False\n",
    "updated_images2 = FaceRec(new_images2, showlog)\n",
    "print(updated_images2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepface with pp\n",
    "import concurrent.futures\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# def load_and_preprocess_image(image_path):\n",
    "#     # Load image using TensorFlow\n",
    "#     image = tf.io.read_file(image_path)\n",
    "#     image = tf.image.decode_image(image)\n",
    "#     # Preprocess image if necessary (resize, normalize, etc.)\n",
    "#     image = tf.image.resize(image, [224, 224])  # Example resize to 224x224\n",
    "#     image = image / 255.0  # Example normalization\n",
    "#     return image\n",
    "\n",
    "def process_image(image_data, silent):\n",
    "    image_path, coords = image_data\n",
    "    if not silent:\n",
    "        print(\"\\n----------------------------------------------\\n\")\n",
    "        print(f\"Image {image_path} : \\n\")\n",
    "        newfindings = NFR.FindFaceFromImage(image_path, silent=silent)\n",
    "        print(f\"new finding:\\n{newfindings}\\n\")\n",
    "        print(\"\\n----------------------------------------------\\n\")\n",
    "    else : \n",
    "        newfindings = NFR.FindFaceFromImage(image_path,silent=silent)\n",
    "    return (image_path, coords, newfindings)\n",
    "\n",
    "def FaceRecPP(new_images2, silent):\n",
    "    updated_images = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_image = {executor.submit(process_image, image_data, silent): image_data for image_data in new_images2}\n",
    "        for future in concurrent.futures.as_completed(future_to_image):\n",
    "            updated_images.append(future.result())\n",
    "    \n",
    "    return updated_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cropped_image1.jpg', (300, 300, 150, 150), ['Customer4', 3, 41, 210, 210, 213, 251, 0.008378336968868916]), ('cropped_image0.jpg', (62, 237, 212, 38), ['Customer1', 28, 87, 162, 162, 190, 249, 0.00016068678009617532]), ('D://Retinafacetest/dar.jpg', (4, 237, 212, 38), ['Customer2', 0, 0, 474, 315, 474, 315, 0.04721419451671882]), ('cropped_image2.jpg', (4, 237, 212, 38), ['Customer', 0, 0, 0, 0, 0, 0, 0]), ('D://Retinafacetest/three1.jpg', (4, 237, 212, 38), ['Customer', 0, 0, 0, 0, 0, 0, 0]), ('D://Retinafacetest/h.jpg', (4, 237, 212, 38), ['Ali', 0, 0, 574, 685, 574, 685, 0.01307410502027595]), ('D://Retinafacetest/ms.jpg', (4, 237, 212, 38), ['Customer', 0, 0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "new_images22 = [\n",
    "    (\"cropped_image0.jpg\", (62, 237, 212, 38)),\n",
    "    (\"cropped_image1.jpg\", (300, 300, 150, 150)),\n",
    "    (\"cropped_image2.jpg\", (4, 237, 212, 38)),\n",
    "    (\"D://Retinafacetest/ms.jpg\", (4, 237, 212, 38)),\n",
    "    (\"D://Retinafacetest/dar.jpg\", (4, 237, 212, 38)),\n",
    "    (\"D://Retinafacetest/three1.jpg\", (4, 237, 212, 38)),\n",
    "    (\"D://Retinafacetest/h.jpg\", (4, 237, 212, 38)),\n",
    "    \n",
    "    \n",
    "    \n",
    "]\n",
    "updated_images22 = FaceRecPP(new_images22, silent=False)\n",
    "print(updated_images22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    # Load image using TensorFlow\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_image(image, channels=3)  # Ensure 3 color channels (RGB)\n",
    "    # Preprocess image (resize, normalize, etc.)\n",
    "    image = tf.image.resize(image, [224, 224])  # Example resize to 224x224\n",
    "    image = image / 255.0  # Normalize pixel values to [0, 1]\n",
    "    return image\n",
    "\n",
    "def batch_process_images(image_paths, showlog):\n",
    "    images = [load_and_preprocess_image(image_path) for image_path in image_paths]\n",
    "    images_tensor = tf.convert_to_tensor(images)\n",
    "\n",
    "    # Perform batch processing with TensorFlow\n",
    "    # Assuming you have a TensorFlow model for face detection\n",
    "    model = ...  # Load your TensorFlow face detection model\n",
    "    predictions = model.predict(images_tensor)\n",
    "\n",
    "    results = []\n",
    "    for image_path, prediction in zip(image_paths, predictions):\n",
    "        if showlog:\n",
    "            print(f\"\\n----------------------------------------------\\n\")\n",
    "            print(f\"Image {image_path} : \\nPrediction: {prediction}\\n\")\n",
    "            print(\"\\n----------------------------------------------\\n\")\n",
    "        results.append((image_path, prediction))\n",
    "    return results\n",
    "\n",
    "def FaceRecPP(new_images2, showlog):\n",
    "    image_paths = [image_path for image_path, _ in new_images2]\n",
    "    coords_list = [coords for _, coords in new_images2]\n",
    "\n",
    "    # Batch process all images\n",
    "    batch_results = batch_process_images(image_paths, showlog)\n",
    "\n",
    "    updated_images = []\n",
    "    for (image_path, coords), (pred_path, prediction) in zip(new_images2, batch_results):\n",
    "        updated_images.append((image_path, coords, prediction))\n",
    "    \n",
    "    return updated_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m\n\u001b[0;32m      1\u001b[0m new_images222 \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcropped_image0.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;241m62\u001b[39m, \u001b[38;5;241m237\u001b[39m, \u001b[38;5;241m212\u001b[39m, \u001b[38;5;241m38\u001b[39m)),\n\u001b[0;32m      3\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcropped_image1.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m150\u001b[39m)),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \n\u001b[0;32m     12\u001b[0m ]\n\u001b[0;32m     13\u001b[0m showlog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m updated_images222 \u001b[38;5;241m=\u001b[39m \u001b[43mFaceRecPP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_images222\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshowlog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(updated_images222)\n",
      "Cell \u001b[1;32mIn[6], line 33\u001b[0m, in \u001b[0;36mFaceRecPP\u001b[1;34m(new_images2, showlog)\u001b[0m\n\u001b[0;32m     30\u001b[0m coords_list \u001b[38;5;241m=\u001b[39m [coords \u001b[38;5;28;01mfor\u001b[39;00m _, coords \u001b[38;5;129;01min\u001b[39;00m new_images2]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Batch process all images\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m batch_results \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_process_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshowlog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m updated_images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (image_path, coords), (pred_path, prediction) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(new_images2, batch_results):\n",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m, in \u001b[0;36mbatch_process_images\u001b[1;34m(image_paths, showlog)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Perform batch processing with TensorFlow\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Assuming you have a TensorFlow model for face detection\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m  \u001b[38;5;66;03m# Load your TensorFlow face detection model\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(images_tensor)\n\u001b[0;32m     19\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_path, prediction \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(image_paths, predictions):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "new_images222 = [\n",
    "    (\"cropped_image0.jpg\", (62, 237, 212, 38)),\n",
    "    (\"cropped_image1.jpg\", (300, 300, 150, 150)),\n",
    "    (\"cropped_image2.jpg\", (4, 237, 212, 38)),\n",
    "    (\"D://Retinafacetest/ms.jpg\", (4, 237, 212, 38)),\n",
    "    (\"D://Retinafacetest/dar.jpg\", (4, 237, 212, 38)),\n",
    "    (\"D://Retinafacetest/three1.jpg\", (4, 237, 212, 38)),\n",
    "    (\"D://Retinafacetest/h.jpg\", (4, 237, 212, 38)),\n",
    "    \n",
    "    \n",
    "    \n",
    "]\n",
    "showlog = True\n",
    "updated_images222 = FaceRecPP(new_images222, showlog)\n",
    "print(updated_images222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cropped_image0.jpg   | (62, 237, 212, 38)     | ['Noore', 28, 87, 162, 162, 190, 249, 0.2882234397537323]    \n",
      "cropped_image1.jpg   | (300, 300, 150, 150)   | ['hamza1', 3, 41, 210, 210, 213, 251, 0.30360839521578475]   \n",
      "cropped_image2.jpg   | (4, 237, 212, 38)      | ['aboud', -11, 99, 251, 251, 240, 350, 0.01301719583403782]  \n"
     ]
    }
   ],
   "source": [
    "# finding=FaceRec(showlog=False)\n",
    "# chatgpt format \n",
    "max_lengths = [max(len(str(item)) for item in data_column) for data_column in zip(*updated_images2)]\n",
    "\n",
    "for row in updated_images2:\n",
    "    formatted_row = \" | \".join(f\"{str(item):<{max_length+2}}\" for item, max_length in zip(row, max_lengths))\n",
    "    print(formatted_row)\n",
    "\n",
    "# print(f\"name= {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cropped_image0.jpg', (62, 237, 212, 38), ['Noore', 28, 87, 162, 162, 190, 249, 0.2882234397537323])\n",
      "('cropped_image1.jpg', (300, 300, 150, 150), ['hamza1', 3, 41, 210, 210, 213, 251, 0.30360839521578475])\n",
      "('cropped_image2.jpg', (4, 237, 212, 38), ['aboud', -11, 99, 251, 251, 240, 350, 0.01301719583403782])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i,index in enumerate(updated_images2):\n",
    "    \n",
    "    print(f\"{index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
